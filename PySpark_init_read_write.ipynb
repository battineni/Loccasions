{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark_init_read_write.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/battineni/Loccasions/blob/master/PySpark_init_read_write.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RupnLViTvUc"
      },
      "source": [
        "# Step 1 install Apache Spark 3.2.0 with Hadoop 3.2 from below link \n",
        "# Get download latest from https://spark.apache.org/downloads.html  and update the file name \n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "# Step 2\n",
        "# Unzip and the compressed file:\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "# Next, we need to install and that is the findspark library. \n",
        "# It will locate Spark on the system and import it as a regular library.\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT0vUDv7hUaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24125882-80ce-43ac-f731-88c88c0e1edd"
      },
      "source": [
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "print(os.environ['SPARK_HOME'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-3.2.0-bin-hadoop3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vni1NSljfEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4181075a-e949-4694-9659-83029ecaea34"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        " \n",
        "#only run after findspark,init()\n",
        "\n",
        "# import pyspark\n",
        "import pyspark \n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "#Create SparkSession which creates SparkContext.\n",
        "#not required starting 3.x\n",
        "#sc=spark.sparkContext\n",
        "print(pyspark.__version__)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "yYXe7nZBgLIt",
        "outputId": "67924eea-e229-4be8-c725-765cbb457f8d"
      },
      "source": [
        "# to know the location where Spark is installed, use findspark.find()\n",
        "findspark.find()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.2.0-bin-hadoop3.2'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umy1YNoUhbGM",
        "outputId": "860dd3c7-d1a1-4c84-91b7-896a6f8e2655"
      },
      "source": [
        "\n",
        "# Check the pyspark version\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "7NNfypJwUOX1",
        "outputId": "b27a3e78-f5c1-4254-a068-8bd7b9d9ec4d"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://516f428644e5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f567657f850>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "fY6CpC0TUOhj",
        "outputId": "d121f070-6281-472f-d806-9ca58fa455f5"
      },
      "source": [
        "sc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khUn7dScgDlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bd62f50-0d60-4f56-a4ec-104ad572178b"
      },
      "source": [
        "df = spark.sql('''Select 'Spark' as hello ''')\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|Spark|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCzr5c7ug2I9"
      },
      "source": [
        "# Read CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0nogkDIAjo4",
        "outputId": "4eec8457-d4cc-4d8e-8b30-2ac0aa8157d3"
      },
      "source": [
        "df =spark.read.csv(header=True, inferSchema=True, path=\"/content/sample_data/california_housing_test.csv\")\n",
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "|  -122.05|   37.37|              27.0|     3885.0|         661.0|    1537.0|     606.0|       6.6085|          344700.0|\n",
            "|   -118.3|   34.26|              43.0|     1510.0|         310.0|     809.0|     277.0|        3.599|          176500.0|\n",
            "|  -117.81|   33.78|              27.0|     3589.0|         507.0|    1484.0|     495.0|       5.7934|          270500.0|\n",
            "|  -118.36|   33.82|              28.0|       67.0|          15.0|      49.0|      11.0|       6.1359|          330000.0|\n",
            "|  -119.67|   36.33|              19.0|     1241.0|         244.0|     850.0|     237.0|       2.9375|           81700.0|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0cze5RnUOaP",
        "outputId": "2c544bbf-51ab-41a8-ca81-43ce3c607c37"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_M3stXIEYaN"
      },
      "source": [
        "# pySpark data cleansing exercise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b66Vhuqrq4KG"
      },
      "source": [
        "**Partitions in Spark**\n",
        "\n",
        "Partitioning means that the complete data is not present in a single place. It is divided into multiple chunks and these chunks are placed on different nodes.\n",
        "\n",
        "If you have one partition, Spark will only have a parallelism of one, even if you have thousands of executors. Also, if you have many partitions but only one executor, Spark will still only have a parallelism of one because there is only one computation resource.\n",
        "\n",
        "In Spark, the lower level APIs allow us to define the number of partitions.\n",
        "\n",
        "Let’s take a simple example to understand how partitioning helps us to give faster results. We will create a list of 20 million random numbers between 10 to 1000 and will count the numbers greater than 200."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYphJYM4UOcp",
        "outputId": "05941cae-660f-4a85-b9e4-90d4cc4543ea"
      },
      "source": [
        "from random import randint \n",
        "\n",
        "# create a list of random numbers between 10 to 1000\n",
        "my_large_list = [randint(10,1000) for x in range(0,20000000)]\n",
        "\n",
        "# create one partition of the list  \n",
        "my_large_list_one_partition = sc.parallelize(my_large_list,numSlices=1)\n",
        "\n",
        "# check number of partitions\n",
        "print(my_large_list_one_partition.getNumPartitions())\n",
        "# >> 1\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_one_partition = my_large_list_one_partition.filter(lambda x : x >= 200)\n",
        "\n",
        "# code was run in a jupyter notebook \n",
        "# to calculate the time taken to execute the following command\n",
        "%%time\n",
        "\n",
        "# count the number of elements in filtered list\n",
        "print(my_large_list_one_partition.count())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16165809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSFyFtRKUOfJ"
      },
      "source": [
        "# create five partitions of the list\n",
        "my_large_list_with_five_partition = sc.parallelize(my_large_list, numSlices=5)\n",
        "\n",
        "# filter numbers greater than equal to 200\n",
        "my_large_list_with_five_partition = my_large_list_with_five_partition.filter(lambda x : x >= 200)\n",
        "\n",
        "%%timeit \n",
        "# count the number of elements in the filtered list\n",
        "print(my_large_list_with_five_partition.count())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvnPHqsGUOj6",
        "outputId": "d5ebc174-1e09-4cd9-b939-53cc132347e9"
      },
      "source": [
        "nums = list(range(0,1000001))\n",
        "len(nums)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000001"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXfS4MljUOmZ",
        "outputId": "3db3ca49-6126-4afd-b473-8fa56f864f59"
      },
      "source": [
        "# to distribute a large file in to RDD \n",
        "nums_rdd=sc.parallelize(nums)\n",
        "nums_rdd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[30] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yECXun5AUOo3",
        "outputId": "ae24cbef-df0b-45ff-8eab-6c122d3e35d7"
      },
      "source": [
        "# collet is very heavy operation so use take to show only 5 \n",
        "nums_rdd.collect()\n",
        "\n",
        "nums_rdd.take(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDXsvXFWUOrh",
        "outputId": "bd49cfa6-70f9-4546-d615-f0b4c56925a7"
      },
      "source": [
        "# apply a funchtion to all elements of RDD\n",
        "Squared_nums_rdd= nums_rdd.map(lambda x: x**2)\n",
        "Squared_nums_rdd.take(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 4, 9, 16]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54OCLEeFDyKQ"
      },
      "source": [
        "# to print the pairs \n",
        "pairs = Squared_nums_rdd.map(lambda x: (x,len(str(x))))\n",
        "pairs.take(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oigkdtr6Dyhg"
      },
      "source": [
        "# filter function\n",
        "even_digit_pairs = pairs.filter(lambda x: (x[1] % 2) == 0)\n",
        "even_digit_pairs.take(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEKxrX-lDyli"
      },
      "source": [
        "# to flip the list \n",
        "fliped_even_digit_pairs = even_digit_pairs.map(lambda x: (x[1],x[0]))\n",
        "fliped_even_digit_pairs.take(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHIC5mD3Jhdy"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "\n",
        "1.   Structure of Spark's Data Source API\n",
        "        - Read API Structure \n",
        "        - Write API Structure  \n",
        "2.   Apache Spark Data Sources you Should Know About\n",
        "      \n",
        "        - CSV, JSON, Parquet, ORC, Text, JDBC/ODBC Connections \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdadEErhQtQZ"
      },
      "source": [
        "# DataFrameReader\n",
        "\n",
        "```\n",
        "   # spark.read.format(\"csv\")\n",
        "      .option(\"mode\", \"FAILFAST\")\n",
        "      .option(\"inferSchema\", \"true\")\n",
        "      .option(\"path\", \"path/to/file(s)\")\n",
        "      .schema(someSchema)\n",
        "      .load()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6vV9jfeTA3Y"
      },
      "source": [
        "# WritingData\n",
        "\n",
        "```\n",
        "    dataframe.write.format(\"csv\")\n",
        "      .option(\"mode\", \"OVERWRITE\")\n",
        "      .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
        "      .option(\"path\", \"path/to/file(s)\")\n",
        "      .save()\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jBGP-kSTXOs"
      },
      "source": [
        "# .format specified hpw the file needs to be written \n",
        "# .option is optional as Spark uses parquet by default\n",
        "# .PartitionBy, .bucketBy, .sortBy are only used with file-based data sources and control the file structure\n",
        "# otr layout at the destination"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9cZ54itVfvb"
      },
      "source": [
        "# https://www.analyticsvidhya.com/blog/2020/10/data-engineering-101-data-sources-apache-spark/?utm_source=blog&utm_medium=working-with-pyspark-on-google-colab-for-data-scientists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW_kZ-4qVcmj"
      },
      "source": [
        "# Save Modes \n",
        "\n",
        "     -  append\tAppends the output files to the list of files that already exist at that location\n",
        "     -  overwrite\tWill completely overwrite any data that already exists there\n",
        "     -  errorIfExists\tThrows an error and fails the write if data or files already exist at the specified location\n",
        "     -  ignore\tIf data or files exist at the location, do nothing with the current DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oWxt_ywQwFX"
      },
      "source": [
        "dataframe.write.format(\"csv\")\n",
        ".option(\"mode\", \"OVERWRITE\")\n",
        ".option(\"dateFormat\", \"yyyy-MM-dd\")\n",
        ".option(\"path\", \"path/to/file(s)\")\n",
        ".save()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N70VQZNADypU",
        "outputId": "7528ba15-62ae-458f-826a-01e368f1ed98"
      },
      "source": [
        "# this will create spark dataframe\n",
        "df = spark.read.csv(header=True, inferSchema=True,path=\"/content/train.csv\")\n",
        "df.take(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(User_ID=1000001, Product_ID='P00069042', Gender='F', Age='0-17', Occupation=10, City_Category='A', Stay_In_Current_City_Years='2', Marital_Status=0, Product_Category_1=3, Product_Category_2=None, Product_Category_3=None, Purchase=8370),\n",
              " Row(User_ID=1000001, Product_ID='P00248942', Gender='F', Age='0-17', Occupation=10, City_Category='A', Stay_In_Current_City_Years='2', Marital_Status=0, Product_Category_1=1, Product_Category_2=6, Product_Category_3=14, Purchase=15200),\n",
              " Row(User_ID=1000001, Product_ID='P00087842', Gender='F', Age='0-17', Occupation=10, City_Category='A', Stay_In_Current_City_Years='2', Marital_Status=0, Product_Category_1=12, Product_Category_2=None, Product_Category_3=None, Purchase=1422),\n",
              " Row(User_ID=1000001, Product_ID='P00085442', Gender='F', Age='0-17', Occupation=10, City_Category='A', Stay_In_Current_City_Years='2', Marital_Status=0, Product_Category_1=12, Product_Category_2=14, Product_Category_3=None, Purchase=1057),\n",
              " Row(User_ID=1000002, Product_ID='P00285442', Gender='M', Age='55+', Occupation=16, City_Category='C', Stay_In_Current_City_Years='4+', Marital_Status=0, Product_Category_1=8, Product_Category_2=None, Product_Category_3=None, Purchase=7969)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_vx3_73kpsr"
      },
      "source": [
        "### Show Column details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHFZh2eKDyti",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504c8e03-3ad2-4171-a064-c05190b9ec5c"
      },
      "source": [
        "# Show column details , first step of exploratory data analysis aka EDA is to check schema of  dataframe\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- User_ID: integer (nullable = true)\n",
            " |-- Product_ID: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Occupation: integer (nullable = true)\n",
            " |-- City_Category: string (nullable = true)\n",
            " |-- Stay_In_Current_City_Years: string (nullable = true)\n",
            " |-- Marital_Status: integer (nullable = true)\n",
            " |-- Product_Category_1: integer (nullable = true)\n",
            " |-- Product_Category_2: integer (nullable = true)\n",
            " |-- Product_Category_3: integer (nullable = true)\n",
            " |-- Purchase: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iezRxUj6kx3r"
      },
      "source": [
        "### Display Rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fid3RhbYDyx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e03adc-f74c-4a8b-90ae-3e30d1718ce1"
      },
      "source": [
        "#Functions\n",
        "df.show(n=5, truncate=True, vertical=False) #like pandas head()\n",
        "df.count() #Number of rows in DF\n",
        "df.select(\"User_ID\",\"Product_ID\").show(5)\n",
        "df.describe().show() #to look at Statistical\n",
        "df.select(\"City_Category\").distinct().show() # Distinct Values for Categorical columns\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|City_Category|\n",
            "+-------------+\n",
            "|            B|\n",
            "|            C|\n",
            "|            A|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOCmqA7lDy2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7502d79d-793f-4c36-bd22-015dbcb6d6a9"
      },
      "source": [
        "#df.groupBy(\"City_Category\").avg().show()\n",
        "df.groupBy(\"City_Category\").sum(\"Purchase\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------------+\n",
            "|City_Category|sum(Purchase)|\n",
            "+-------------+-------------+\n",
            "|            B|   2115533605|\n",
            "|            C|   1663807476|\n",
            "|            A|   1316471661|\n",
            "+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpJdUZrGDy8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c6113a-cbd9-40d7-a4bf-07a84ee5e6fe"
      },
      "source": [
        "df.groupBy(\"City_Category\").count().show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------+\n",
            "|City_Category| count|\n",
            "+-------------+------+\n",
            "|            B|231173|\n",
            "|            C|171175|\n",
            "|            A|147720|\n",
            "+-------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBYq9E57DzAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6124c030-5e5d-46fa-da01-728e76839c3e"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "# df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).show()\n",
        "\n",
        "df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "|User_ID|Product_ID|Gender|Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
            "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "|      0|         0|     0|  0|         0|            0|                         0|             0|                 0|            173638|            383247|       0|\n",
            "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g8x4XmmDzFL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs79jr83DzJz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4OFxTpzDzOt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A47Y-xz_DzTr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMpBTr3ZDzZ1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_xr6nwUDze-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7ZTcx8-Dzly"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSuScQzDDzrC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJtbHpJPDzwb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}